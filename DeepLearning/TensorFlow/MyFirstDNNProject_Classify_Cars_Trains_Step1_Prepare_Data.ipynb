{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This is the Python code developed by Hang Zhang (@HangZhang6) to train a deep learning model to classify images as Cars or Trains. This is developed based on the assignments of course [Deep Learning by Google on UdaCity](https://www.udacity.com/course/deep-learning--ud730)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Crawl Bing.com image search for classes of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice Deep Learning on images, it is fundamental to have images to train DL models.\n",
    "\n",
    "While there are many open-source scraper scripts available online, many of them are obsolete and do not work any more.\n",
    "\n",
    "The [Bulk-Bing-Image-downloader](https://github.com/ostrolucky/Bulk-Bing-Image-downloader) is the one that I found easy to use and works smoothly. I tested it on April 24, 2017.\n",
    "\n",
    "The bbid crawler runs on Python 3.x. \n",
    "\n",
    "To crawl the images impressed for a query keyword on Bing.com, you need to:\n",
    "\n",
    "- Download the [bbid.py](https://raw.githubusercontent.com/ostrolucky/Bulk-Bing-Image-downloader/master/bbid.py) to your local machine. If you are using Windows machine, run the following PowerShell command to download it:\n",
    "    \n",
    "        wget https://raw.githubusercontent.com/ostrolucky/Bulk-Bing-Image-downloader/master/bbid.py -O bbid.py\n",
    "    \n",
    "    \n",
    "- Run the following command to crawl the Bing image search page for a query (e.g., 'Cars') and output the impressed images to a designated directory (e.g., C:\\Images\\Cars):\n",
    "\n",
    "        python bbid.py -s \"Cars\" -o C:\\Images\\Cars --filter\n",
    "    \n",
    "    Parameter --filter filters out the adult content from the search results. \n",
    "\n",
    "- Run it again for another search keyword (e.g., 'Trains') and output the images to another designated directory (e.g., C:\\Images\\Trains):\n",
    "\n",
    "        python bbid.py -s \"Trains\" -o C:\\Images\\Trains --filter \n",
    "        \n",
    "[NOTE] Many times the more training images you have, the more accurate deep learning model you can train. The original bbid.py always starts crawling the search result images from the first result. I did some tiny modification to bbid.py so that you can specify which image to start with. \n",
    "\n",
    "The modified bbid.py is also checked in to my personal github. Users can specify which image to start with by providing a parameter _-p_ in the command line, for instance:\n",
    "\n",
    "        python bbid.py -s \"Trains\" -p 200 -o C:\\Images\\Trains --filter\n",
    "\n",
    "This line will start crawling the search result images from the 200th image. If _-p_ parameter is not provided in the command line, it will start from the first image. \n",
    "\n",
    "In this way, users can run this command multiple times, every time with different values of the _-p_ parameter. The number of training images can be significantly enlarged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "## Step 2: Resize Images\n",
    "\n",
    "The crawled images are of different sizes, or even with different dimensions (some are gray-scaled with only 1 channel, some are 256-color scale with 3 channels). To train DL models on images, we need to resize them to be of the same size. In addition, because of the limitation of the memory on my laptop, I have to downsize the images so that later on the machine has enough memories to store and train the parameters of the DL models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_size(root_dir, folders):\n",
    "    num_images = 0\n",
    "    for folder in folders:\n",
    "        folder = os.path.join(root_dir, folder)\n",
    "        image_files = os.listdir(folder)\n",
    "        num_images += len(image_files)\n",
    "    image_size = np.ndarray(shape=(num_images,2), dtype=np.integer)\n",
    "    img_counter = 0\n",
    "    for folder in folders:\n",
    "        folder = os.path.join(root_dir, folder)\n",
    "        image_files = os.listdir(folder)\n",
    "        for image in image_files:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            try:\n",
    "                image_data = ndimage.imread(image_file).astype(float)\n",
    "                image_size[img_counter,:] = image_data.shape[0:2]\n",
    "                img_counter += 1\n",
    "            except IOError as e:\n",
    "                print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    print(\"Totally %d images are scanned for sizes\"%img_counter)\n",
    "    avg_size = np.average(image_size, axis=0)\n",
    "    print(avg_size)\n",
    "    avg_size = [round(x) for x in avg_size] \n",
    "    return avg_size\n",
    "\n",
    "def resize_image(image_file, width, height):\n",
    "    try:\n",
    "      pixel_depth = 255.0\n",
    "      image = Image.open(image_file)\n",
    "      if image.mode == 'CMYK':\n",
    "        print('Converting image %s from CMYK to RGB'%image_file)\n",
    "        image = image.convert('RGB')\n",
    "      im2 = image.resize((width, height), Image.ANTIALIAS)\n",
    "      im2 = np.array(im2.getdata(),\n",
    "                    np.uint8).reshape(im2.size[1], im2.size[0], 3)\n",
    "      # randomly choose some images to show both before and after resizing, to make sure that \n",
    "      # the resizing is as expected\n",
    "      rn = random.random()\n",
    "      if rn < 0.002:\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        image_data = ndimage.imread(image_file).astype(float)\n",
    "        image_data = image_data / 255\n",
    "        axarr[0].imshow(image_data)\n",
    "        im2 = im2 / 255\n",
    "        axarr[1].imshow(im2)\n",
    "    except:\n",
    "      print(\"Resizing failed on file %s\"%image_file)\n",
    "    return im2\n",
    "\n",
    "def load_resize_images(root_dir, folder, width, height):\n",
    "  \"\"\"Load and resize the images.\"\"\"\n",
    "  pixel_depth = 255.0\n",
    "  folder = os.path.join(root_dir, folder)\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), height, width, 3),\n",
    "                         dtype=np.float16)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    filename, file_extension = os.path.splitext(image_file)\n",
    "    if file_extension in [\".jpg\", \".JPG\", \".jpeg\"]:\n",
    "      try:\n",
    "        image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "        if len(image_data.shape) == 3:\n",
    "          if image_data.shape[2] < 3:\n",
    "            print(\"Image file %s has %d channels, skipping\"%(image_file, image_data.shape[2]))\n",
    "          elif image_data.shape[0] < width or image_data.shape[1] < height:\n",
    "            print(\"Image file %s is too small, skipping\"%image_file)\n",
    "          else:\n",
    "            try:\n",
    "              image_data = resize_image(image_file, width, height)\n",
    "              dataset[num_images, :, :, :] = image_data\n",
    "              num_images = num_images + 1\n",
    "            except:\n",
    "              print(\"Could not resize %s, skipping\"%image_file)\n",
    "        else:\n",
    "          print(\"Image file %s only has %d dimensions, skipping\"%(image_file, len(image_data.shape)))\n",
    "      except IOError as e:\n",
    "        print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    else:\n",
    "      print(\"Image file %s is not a jpg file, skipping\"%image_file)\n",
    "  dataset = dataset[0:num_images, :, :, :]\n",
    "  #dataset = (dataset - pixel_depth/2)/(pixel_depth/2)\n",
    "  print(\"%d images in folder %s are added into the pickle file\"%(num_images, folder))\n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(root_dir, image_folders, width, height, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in image_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    set_filename = os.path.join(root_dir, set_filename)\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      \n",
    "      dataset = load_resize_images(root_dir, folder, width, height)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and resize the images to an uniform and smaller size, and dump to pickles to future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform image size is only 100-by-80 pixels. I chose this size due to the memory limitation on my laptop. Images at gray scale, smaller than the desired target size in either dimension are neglected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_root = 'C:\\Projects\\DX\\Data\\Images' # Change me to store data elsewhere\n",
    "image_dir = ['Cars','Trains']\n",
    "#[width, height] = get_size(data_root, image_dir)\n",
    "#ratio = 0.0001\n",
    "width1 = 100\n",
    "height1 = 80\n",
    "dataset_names = maybe_pickle(data_root, image_dir, width1, height1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Split the data (Cars and Trains) into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "jNWGtZaXn-5j"
   },
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, width, height):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, width, height, 3), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, width, height, train_ratio = 0.7, validation_ratio = 0.15):\n",
    "  num_classes = len(pickle_files)\n",
    "  num_vehicles = [0]*num_classes\n",
    "  for i in range(num_classes):\n",
    "    with open(pickle_files[i], 'rb') as f:\n",
    "      vehicle_set = pickle.load(f)\n",
    "      num_vehicles[i] = vehicle_set.shape[0]\n",
    "  total_vehicles = np.sum(num_vehicles)\n",
    "  num_train = [int(round(float(x)*train_ratio)) for x in num_vehicles]\n",
    "  num_valid = [int(round(float(x)*validation_ratio)) for x in num_vehicles]\n",
    "  num_test = np.array(num_vehicles) - np.array(num_train) - np.array(num_valid)\n",
    "  total_train = np.sum(num_train)\n",
    "  total_valid = np.sum(num_valid)\n",
    "  total_test = np.sum(num_test)\n",
    "  print(\"There are %d, %d, %d train, validation, and test images.\"%(total_train, total_valid, total_test))\n",
    "  test_dataset, test_labels = make_arrays(total_test, width, height)\n",
    "  valid_dataset, valid_labels = make_arrays(total_valid, width, height)\n",
    "  train_dataset, train_labels = make_arrays(total_train, width, height)\n",
    "    \n",
    "  start_v, start_trn, start_tst = 0, 0, 0\n",
    "  for label, pickle_file in enumerate(pickle_files): # the first element in pickle_files will be label 0, \n",
    "                                                     # the second with label 1, etc      \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        vehicle_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(vehicle_set)\n",
    "        train_vehicle = vehicle_set[0:num_train[label], :, :, :]\n",
    "        train_dataset[start_trn:(start_trn+num_train[label]), :, :, :] = train_vehicle\n",
    "        train_labels[start_trn:(start_trn+num_train[label])] = label\n",
    "        start_trn += num_train[label]\n",
    "                    \n",
    "        valid_vehicle = vehicle_set[num_train[label]:(num_valid[label]+num_train[label]), :, :, :]\n",
    "        valid_dataset[start_v:(start_v+num_valid[label]), :, :, :] = valid_vehicle\n",
    "        valid_labels[start_v:(start_v+num_valid[label])] = label\n",
    "        start_v += num_valid[label]\n",
    "        \n",
    "        test_vehicle = vehicle_set[(num_valid[label]+num_train[label]):num_vehicles[label], :, :, :]\n",
    "        test_dataset[start_tst:(start_tst+num_test[label]), :, :, :] = test_vehicle\n",
    "        test_labels[start_tst:(start_tst+num_test[label])] = label\n",
    "        start_tst += num_test[label]\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels\n",
    "            \n",
    "            \n",
    "train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = merge_datasets(\n",
    "  dataset_names, height1, width1)\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize two classes of images in each dataset\n",
    "import matplotlib.image as mpimg\n",
    "def visualize_two_class_images(dataset):\n",
    "    f, axarr = plt.subplots(1, 2)\n",
    "    image_data = dataset[0,:,:,:]\n",
    "    image_data = image_data / 255\n",
    "    axarr[0].imshow(image_data, interpolation='none', aspect='auto')\n",
    "    image_data = dataset[-1,:,:,:]\n",
    "    image_data = image_data / 255\n",
    "    axarr[1].imshow(image_data, interpolation='none', aspect='auto')\n",
    "\n",
    "visualize_two_class_images(train_dataset)\n",
    "visualize_two_class_images(valid_dataset)\n",
    "visualize_two_class_images(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Save both training, validation, and testing data into a single pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'carsTrains.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
